


#used to get the list of duplicate files
find_dups(){
echo "Input Directory"
read source
find $source -type f -exec md5sum "{}" >>/tmp/$source"_md5s"
"

cat $1 |cut -d" " -f1|sort |uniq -c | sort -n |grep -v " 1 " >> /media/Knowledge/INCOMINGMDSdate_Duplicates
cat /media/Knowledge/INCOMINGMDSdate_Duplicates

#another example more precise
less /media/Knowledge/Incoming_Storage_md5s_Tue_Sep_27_21\:17\:55_UTC_2016 |cut -d" " -f1|sort | uniq -c | sort -n|grep -v " 1 "|sed -re 's/^\s+//g'

}

#fnames duplicates md5 list
cat $fnames|cut -d" " -f1 |sort |uniq -c | sort -n|grep -v " 1 " |sed -re 's/^\s+//g'|cut -d" " -f2>"$fnames"_dupes

# This is working ############# use the name fnames and fnames_dups
for line in $(cat "$fnames"_dupes);do rm_dups $fnames $line;done


#use this to remove duplicate files
rm_dups(){
#echo $1 is the filename $2 is the md5sum of the files to be removed"
cat "$1" |grep "$2">/tmp/rmf
rm $(head -n $(echo $(less /tmp/rmf | wc -l) -1 | bc) /tmp/rmf |cut -d" " -f2-|sed -re 's/\^s//g')
}

#used to get the file of dups cleaned
cat /media/Knowledge/INCOMINGMDSdate_Duplicates |sed -re 's/^\s/g'|cut -d" " -f2 > /media/Knowledge/INCOMING_REMOVE_FILE

#loop over remove file
for line in $(cat /media/Knowledge/INCOMING_REMOVE_FILE);do echo $line;done

#remove all duplicates from the remove file
for line in $(cat /media/Knowledge/INCOMING_REMOVE_FILE);do rm_dups /media/Knowledge/INCOMINGMDSNEW $line;done

#git clone --depth 1 https://github.com/junegunn/fzf.git ~/.fzf
#~/.fzf/install


replace(){
cat $1
i="True"
echo "Q to quit"
delete_patterns=""
while [ "$i" != "Q" ]
do
read i
delete_patterns+="$i|"
echo $delete_patterns
sed -re 's/"'${delete_patterns::-1}'"//g' $1 | tee $1_cleaned 
done

}


loop(){
for line in $(cat $1);do $2 $line;done
}

split_lines(){
pg $1 | sed -re 's/\./\n/g'|sed -re 's/^\s+//g'
}

url ()
{
    pg $1 | grep -P "http.*?(\s|$)|\w+\.(com|org|net|gov).*?(\s|$)" -o | sort
}

yc_url_stats(){
url $1 |sed -re 's/&#x2F;/\//g'|grep -vP "<|algolia|http "|sort |uniq -c|sort -n
}

yc_people ()
{
    cat $1 | grep  ago | sed -re 's/^\s+//g' | cut -d' ' -f1 | sort | uniq -c | sort -n
}

yc_people_filter ()
{  cat $1 | grep --color=auto ago | sed -re 's/^\s+//g' | cut -d' ' -f1 | sort | uniq -c | sort -n>people_stats
   cat people_stats
   
   cat people_stats | tail -n $2 | sed -re 's/^\s+//g' | cut -d' ' -f2 | tee people
   grep -f people -A 10 $1
}

vocab(){ 
pg $1 | sed -re 's/\./\n/g'|sed -re 's/^\s//g'|sed -re 's/\s/\n/g'|grep -ivwf stop_words|sed -re 's/\,|:|\(|\)|\[|\]|\"|\?//g'|grep -v "[0-9]"|sed -n '/.\{5\}/p'|sed -re "s/^'|\-\-$|;//g" 
}

frequency(){
sort|uniq -c | sort -n
}

frequency_filter(){
tail -n $1|sed -re 's/^\s+//g' | cut -d' ' -f2
}

numbers(){
pg $1 | grep -P "[0-9]+(\.[0-9])*"
}

general_highlighting(){
cat $1|grep -P "https*.*?(\s|\$)|\$|[a-z]*[A-ZÃ¼\']+([a-z\']*[A-Z\']*)*|[0-9\.]+|\(.*?\)|\!|\[.*?\]|\=|%|\w+\:|\w+(ed\s|ing\s)"  --color=always|more
}


ngram(){
grep -iP "(\s*\w+\s*){$3}$1.(\s*\w+\s*){$4}" $2
}

word_count(){
pg $1|sed -re 's/\s/\n/g'|sort|sed -re 's/^$|\W|[0-9]|\s+//g'|frequency |more|grep -P "[a-zA-Z]"|sed -re 's/^\s+//g'
}


add_columns(){
paste -sd+ - | bc
}



edits(){
echo "input search key term and filename"
output=$(grep -niP $1 $(find . -name "*.$2")|fzf)
filename=$(echo $output |cut -d':' -f1)
linenumber=$(echo $output |cut -d':' -f2)
nano +$linenumber $filename
}



url_go(){
test_var=$(url $1|fzf)
file_name=$(echo $test_var|rev|cut -d'/' -f1|rev)
#echo $file_name
w3m $test_var > $file_name
firefox $test_var
}

stats(){
sort|uniq -c |sort -n
}

freq_filter(){
echo $0
tail -n $1|sed -re 's/^\s+//g'|cut -d' ' -f2 > frequent_words
grep -f frequent_words $1
}

#http://cs224d.stanford.edu/syllabus.html
#for line in $(less ../syllabus.html |grep -P http.*?\" -o| tr -d '"' |grep pdf );do wget $line;done

#backup firefox history
#cd ~/.mozilla/firefox/$(ls ~/.mozilla/firefox/|grep default)
#grep 'http' -r .| grep "wiki"|sed -re 's/\{/\n/g'|grep url.*?\, -Po|grep http.* -oP|sed -re 's/"|\,//g'|sort | uniq -c | sort -n> browser_history_"$(date|sed -re 's/\s+/_/g')"

domains(){
less $1 |grep -oP "http.*" |sed -re 's/\s+.*//g'|sed -re 's/https?:\/\///g'|sed -re 's/\/.*//g'|stats
}
