
#use this to remove duplicate files
rm_dups(){
#echo $1 is the filename $2 is the md5sum of the files to be removed"
cat "$1" |grep "$2">/tmp/rmf
rm $(head -n $(echo $(less /tmp/rmf | wc -l) -1 | bc) /tmp/rmf |cut -d" " -f2-|sed -re 's/\^s//g')
}

replace(){
cat $1
i="True"
echo "Q to quit"
delete_patterns=""
while [ "$i" != "Q" ]
do
read i
delete_patterns+="$i|"
echo $delete_patterns
sed -re 's/"'${delete_patterns::-1}'"//g' $1 | tee $1_cleaned 
done

}


url ()
{
    pg $1 | grep -P "http.*?(\s|$)|\w+\.(com|org|net|gov).*?(\s|$)" -o | sort
}

yc_url_stats(){
url $1 |sed -re 's/&#x2F;/\//g'|grep -vP "<|algolia|http "|sort |uniq -c|sort -n
}

yc_people ()
{
    cat $1 | grep  ago | sed -re 's/^\s+//g' | cut -d' ' -f1 | sort | uniq -c | sort -n
}

yc_people_filter ()
{  cat $1 | grep --color=auto ago | sed -re 's/^\s+//g' | cut -d' ' -f1 | sort | uniq -c | sort -n>people_stats
   cat people_stats
   
   cat people_stats | tail -n $2 | sed -re 's/^\s+//g' | cut -d' ' -f2 | tee people
   grep -f people -A 10 $1
}

vocab(){ 
pg $1 | sed -re 's/\./\n/g'|sed -re 's/^\s//g'|sed -re 's/\s/\n/g'|grep -ivwf stop_words|sed -re 's/\,|:|\(|\)|\[|\]|\"|\?//g'|grep -v "[0-9]"|sed -n '/.\{5\}/p'|sed -re "s/^'|\-\-$|;//g" 
}



numbers(){
pg $1 | grep -P "[0-9]+(\.[0-9])*"
}

general_highlighting(){
cat $1|grep -P "https*.*?(\s|\$)|\$|[a-z]*[A-ZÃ¼\']+([a-z\']*[A-Z\']*)*|[0-9\.]+|\(.*?\)|\!|\[.*?\]|\=|%|\w+\:|\w+(ed\s|ing\s)"  --color=always|more
}


ngram(){
grep -iP "(\s*\w+\s*){$3}$1.(\s*\w+\s*){$4}" $2
}

word_count(){
pg $1|sed -re 's/\s/\n/g'|sort|sed -re 's/^$|\W|[0-9]|\s+//g'|frequency |more|grep -P "[a-zA-Z]"|sed -re 's/^\s+//g'
}


add_columns(){
paste -sd+ - | bc
}



edits(){
echo "input search key term and filename"
output=$(grep -niP $1 $(find . -name "*.$2")|fzf)
filename=$(echo $output |cut -d':' -f1)
linenumber=$(echo $output |cut -d':' -f2)
nano +$linenumber $filename
}



url_go(){
test_var=$(url $1|fzf)
file_name=$(echo $test_var|rev|cut -d'/' -f1|rev)
#echo $file_name
w3m $test_var > $file_name
firefox $test_var
}

stats(){
sort|uniq -c |sort -n
}

freq_filter(){
echo $0
tail -n $1|sed -re 's/^\s+//g'|cut -d' ' -f2 > frequent_words
grep -f frequent_words $1
}

#http://cs224d.stanford.edu/syllabus.html
#for line in $(less ../syllabus.html |grep -P http.*?\" -o| tr -d '"' |grep pdf );do wget $line;done

#backup firefox history
#cd ~/.mozilla/firefox/$(ls ~/.mozilla/firefox/|grep default)
#grep 'http' -r .| grep "wiki"|sed -re 's/\{/\n/g'|grep url.*?\, -Po|grep http.* -oP|sed -re 's/"|\,//g'|sort | uniq -c | sort -n> browser_history_"$(date|sed -re 's/\s+/_/g')"

domains(){
less $1 |grep -oP "http.*" |sed -re 's/\s+.*//g'|sed -re 's/https?:\/\///g'|sed -re 's/\/.*//g'|stats
}
